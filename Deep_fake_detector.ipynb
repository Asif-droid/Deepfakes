{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["https://www.kaggle.com/code/anonnosingharay/deepfake-video/notebook\n","\n","https://www.kaggle.com/datasets/sanikatiwarekar/deep-fake-detection-dfd-entire-original-dataset/code?datasetId=5524489&sortBy=voteCount\n","\n","\n","xceptionnet:\n","\n","https://github.com/ondyari/FaceForensics/blob/master/classification/network/models.py\n","\n","https://huggingface.co/blog/prithivMLmods/siglip2-finetune-image-classification\n","\n","paper:\n","https://arxiv.org/html/2503.15867v1"],"metadata":{"id":"AvWMxIRKvAdJ"}},{"cell_type":"markdown","source":["# Download"],"metadata":{"id":"jWBaTDSFPKdm"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"nv8Y1FXqo_yu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1752208010319,"user_tz":-360,"elapsed":23535,"user":{"displayName":"K M Asifur Rahman","userId":"04755396723086649626"}},"outputId":"ffbaa859-9c49-4203-ff3f-81e58daa86d1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["from google.colab import userdata\n","import os\n","\n","os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')\n","os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')"],"metadata":{"id":"W_fcelPxL16_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!kaggle datasets download -d deepfake-detection-challenge"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kW7quBR-NX9H","executionInfo":{"status":"ok","timestamp":1751823075558,"user_tz":-360,"elapsed":1750,"user":{"displayName":"K M Asifur Rahman","userId":"04755396723086649626"}},"outputId":"cd2831a4-2259-410f-f215-a1b9b7f30357"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["403 Client Error: Forbidden for url: https://www.kaggle.com/api/v1/datasets/metadata/kmasifurrahman/deepfake-detection-challenge\n"]}]},{"cell_type":"code","source":["!kaggle competitions download -c deepfake-detection-challenge"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jovQUCHRzEBh","executionInfo":{"status":"ok","timestamp":1751823717484,"user_tz":-360,"elapsed":217753,"user":{"displayName":"K M Asifur Rahman","userId":"04755396723086649626"}},"outputId":"8d85a3d6-dbf7-4904-eff8-e29e55b5120d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading deepfake-detection-challenge.zip to /content\n","100% 4.12G/4.13G [00:26<00:00, 145MB/s]\n","100% 4.13G/4.13G [00:26<00:00, 165MB/s]\n"]}]},{"cell_type":"code","source":["#!/usr/bin/env python\n","\"\"\" Downloads FaceForensics++ and Deep Fake Detection public data release\n","Example usage:\n","    see -h or https://github.com/ondyari/FaceForensics\n","\"\"\"\n","# -*- coding: utf-8 -*-\n","import argparse\n","import os\n","import urllib\n","import urllib.request\n","import tempfile\n","import time\n","import sys\n","import json\n","import random\n","from tqdm import tqdm\n","from os.path import join\n","\n","\n","# URLs and filenames\n","FILELIST_URL = 'misc/filelist.json'\n","DEEPFEAKES_DETECTION_URL = 'misc/deepfake_detection_filenames.json'\n","DEEPFAKES_MODEL_NAMES = ['decoder_A.h5', 'decoder_B.h5', 'encoder.h5',]\n","\n","# Parameters\n","DATASETS = {\n","    'original_youtube_videos': 'misc/downloaded_youtube_videos.zip',\n","    'original_youtube_videos_info': 'misc/downloaded_youtube_videos_info.zip',\n","    'original': 'original_sequences/youtube',\n","    'DeepFakeDetection_original': 'original_sequences/actors',\n","    'Deepfakes': 'manipulated_sequences/Deepfakes',\n","    'DeepFakeDetection': 'manipulated_sequences/DeepFakeDetection',\n","    'Face2Face': 'manipulated_sequences/Face2Face',\n","    'FaceShifter': 'manipulated_sequences/FaceShifter',\n","    'FaceSwap': 'manipulated_sequences/FaceSwap',\n","    'NeuralTextures': 'manipulated_sequences/NeuralTextures'\n","    }\n","ALL_DATASETS = ['original', 'DeepFakeDetection_original', 'Deepfakes',\n","                'DeepFakeDetection', 'Face2Face', 'FaceShifter', 'FaceSwap',\n","                'NeuralTextures']\n","COMPRESSION = ['raw', 'c23', 'c40']\n","TYPE = ['videos', 'masks', 'models']\n","SERVERS = ['EU', 'EU2', 'CA']\n","\n","\n","def parse_args():\n","    parser = argparse.ArgumentParser(\n","        description='Downloads FaceForensics v2 public data release.',\n","        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n","    )\n","    parser.add_argument('output_path', type=str, help='Output directory.')\n","    parser.add_argument('-d', '--dataset', type=str, default='all',\n","                        help='Which dataset to download, either pristine or '\n","                             'manipulated data or the downloaded youtube '\n","                             'videos.',\n","                        choices=list(DATASETS.keys()) + ['all']\n","                        )\n","    parser.add_argument('-c', '--compression', type=str, default='raw',\n","                        help='Which compression degree. All videos '\n","                             'have been generated with h264 with a varying '\n","                             'codec. Raw (c0) videos are lossless compressed.',\n","                        choices=COMPRESSION\n","                        )\n","    parser.add_argument('-t', '--type', type=str, default='videos',\n","                        help='Which file type, i.e. videos, masks, for our '\n","                             'manipulation methods, models, for Deepfakes.',\n","                        choices=TYPE\n","                        )\n","    parser.add_argument('-n', '--num_videos', type=int, default=None,\n","                        help='Select a number of videos number to '\n","                             \"download if you don't want to download the full\"\n","                             ' dataset.')\n","    parser.add_argument('--server', type=str, default='EU',\n","                        help='Server to download the data from. If you '\n","                             'encounter a slow download speed, consider '\n","                             'changing the server.',\n","                        choices=SERVERS\n","                        )\n","    args = parser.parse_args()\n","\n","    # URLs\n","    server = args.server\n","    if server == 'EU':\n","        server_url = 'http://canis.vc.in.tum.de:8100/'\n","    elif server == 'EU2':\n","        server_url = 'http://kaldir.vc.in.tum.de/faceforensics/'\n","    elif server == 'CA':\n","        server_url = 'http://falas.cmpt.sfu.ca:8100/'\n","    else:\n","        raise Exception('Wrong server name. Choices: {}'.format(str(SERVERS)))\n","    args.tos_url = server_url + 'webpage/FaceForensics_TOS.pdf'\n","    args.base_url = server_url + 'v3/'\n","    args.deepfakes_model_url = server_url + 'v3/manipulated_sequences/' + \\\n","                               'Deepfakes/models/'\n","\n","    return args\n","\n","\n","def download_files(filenames, base_url, output_path, report_progress=True):\n","    os.makedirs(output_path, exist_ok=True)\n","    if report_progress:\n","        filenames = tqdm(filenames)\n","    for filename in filenames:\n","        download_file(base_url + filename, join(output_path, filename))\n","\n","\n","def reporthook(count, block_size, total_size):\n","    global start_time\n","    if count == 0:\n","        start_time = time.time()\n","        return\n","    duration = time.time() - start_time\n","    progress_size = int(count * block_size)\n","    speed = int(progress_size / (1024 * duration))\n","    percent = int(count * block_size * 100 / total_size)\n","    sys.stdout.write(\"\\rProgress: %d%%, %d MB, %d KB/s, %d seconds passed\" %\n","                     (percent, progress_size / (1024 * 1024), speed, duration))\n","    sys.stdout.flush()\n","\n","\n","def download_file(url, out_file, report_progress=False):\n","    out_dir = os.path.dirname(out_file)\n","    if not os.path.isfile(out_file):\n","        fh, out_file_tmp = tempfile.mkstemp(dir=out_dir)\n","        f = os.fdopen(fh, 'w')\n","        f.close()\n","        if report_progress:\n","            urllib.request.urlretrieve(url, out_file_tmp,\n","                                       reporthook=reporthook)\n","        else:\n","            urllib.request.urlretrieve(url, out_file_tmp)\n","        os.rename(out_file_tmp, out_file)\n","    else:\n","        tqdm.write('WARNING: skipping download of existing file ' + out_file)\n","\n","\n","def main(args):\n","    # TOS\n","    print('By pressing any key to continue you confirm that you have agreed '\\\n","          'to the FaceForensics terms of use as described at:')\n","    print(args.tos_url)\n","    print('***')\n","    print('Press any key to continue, or CTRL-C to exit.')\n","    _ = input('')\n","\n","    # Extract arguments\n","    c_datasets = [args.dataset] if args.dataset != 'all' else ALL_DATASETS\n","    c_type = args.type\n","    c_compression = args.compression\n","    num_videos = args.num_videos\n","    output_path = args.output_path\n","    os.makedirs(output_path, exist_ok=True)\n","\n","    # Check for special dataset cases\n","    for dataset in c_datasets:\n","      dataset_path = DATASETS[dataset]\n","      # Special cases\n","      if 'original_youtube_videos' in dataset:\n","          # Here we download the original youtube videos zip file\n","          print('Downloading original youtube videos.')\n","          if not 'info' in dataset_path:\n","              print('Please be patient, this may take a while (~40gb)')\n","              suffix = ''\n","          else:\n","              suffix = 'info'\n","          download_file(args.base_url + '/' + dataset_path, out_file=join(output_path,'downloaded_videos{}.zip'.format(suffix)),report_progress=True)\n","          return\n","\n","        # Else: regular datasets\n","      print('Downloading {} of dataset \"{}\"'.format(\n","          c_type, dataset_path\n","      ))\n","\n","      # Get filelists and video lenghts list from server\n","      if 'DeepFakeDetection' in dataset_path or 'actors' in dataset_path:\n","        filepaths = json.loads(urllib.request.urlopen(args.base_url + '/' +\n","              DEEPFEAKES_DETECTION_URL).read().decode(\"utf-8\"))\n","        if 'actors' in dataset_path:\n","          filelist = filepaths['actors']\n","        else:\n","          filelist = filepaths['DeepFakesDetection']\n","      elif 'original' in dataset_path:\n","          # Load filelist from server\n","          file_pairs = json.loads(urllib.request.urlopen(args.base_url + '/' +\n","              FILELIST_URL).read().decode(\"utf-8\"))\n","          filelist = []\n","          for pair in file_pairs:\n","            filelist += pair\n","      else:\n","          # Load filelist from server\n","          file_pairs = json.loads(urllib.request.urlopen(args.base_url + '/' +\n","              FILELIST_URL).read().decode(\"utf-8\"))\n","          # Get filelist\n","          filelist = []\n","          for pair in file_pairs:\n","              filelist.append('_'.join(pair))\n","              if c_type != 'models':\n","                  filelist.append('_'.join(pair[::-1]))\n","      # Maybe limit number of videos for download\n","      if num_videos is not None and num_videos > 0:\n","        print('Downloading the first {} videos'.format(num_videos))\n","        filelist = filelist[:num_videos]\n","\n","      # Server and local paths\n","      dataset_videos_url = args.base_url + '{}/{}/{}/'.format(\n","          dataset_path, c_compression, c_type)\n","      dataset_mask_url = args.base_url + '{}/{}/videos/'.format(\n","          dataset_path, 'masks', c_type)\n","\n","      if c_type == 'videos':\n","          dataset_output_path = join(output_path, dataset_path, c_compression,\n","                                      c_type)\n","          print('Output path: {}'.format(dataset_output_path))\n","          filelist = [filename + '.mp4' for filename in filelist]\n","          download_files(filelist, dataset_videos_url, dataset_output_path)\n","      elif c_type == 'masks':\n","          dataset_output_path = join(output_path, dataset_path, c_type,\n","                                      'videos')\n","          print('Output path: {}'.format(dataset_output_path))\n","          if 'original' in dataset:\n","              if args.dataset != 'all':\n","                  print('Only videos available for original data. Aborting.')\n","                  return\n","              else:\n","                  print('Only videos available for original data. '\n","                        'Skipping original.\\n')\n","                  continue\n","          if 'FaceShifter' in dataset:\n","              print('Masks not available for FaceShifter. Aborting.')\n","              return\n","          filelist = [filename + '.mp4' for filename in filelist]\n","          download_files(filelist, dataset_mask_url, dataset_output_path)\n","\n","      # Else: models for deepfakes\n","      else:\n","          if dataset != 'Deepfakes' and c_type == 'models':\n","              print('Models only available for Deepfakes. Aborting')\n","              return\n","          dataset_output_path = join(output_path, dataset_path, c_type)\n","          print('Output path: {}'.format(dataset_output_path))\n","\n","          # Get Deepfakes models\n","          for folder in tqdm(filelist):\n","              folder_filelist = DEEPFAKES_MODEL_NAMES\n","\n","              # Folder paths\n","              folder_base_url = args.deepfakes_model_url + folder + '/'\n","              folder_dataset_output_path = join(dataset_output_path,\n","                                                folder)\n","              download_files(folder_filelist, folder_base_url,\n","                              folder_dataset_output_path,\n","                              report_progress=False)   # already done\n","\n","\n"],"metadata":{"id":"isxtYxNpvKoy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import sys\n","\n","sys.argv = [\n","    'colab_kernel_launcher.py',                 # dummy script name\n","    '-d', 'Deepfakes',             # dataset\n","    '-c', 'c23',                   # compression\n","    '-t', 'models',                # target type\n","    '-n', '200',                    # number of videos\n","    '--server', 'EU2',             # server\n","    '/content/drive/MyDrive/fake_generated_data/face_forensics/'      # output_path (positional)\n","]\n","args = parse_args()\n","main(args)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lS-mnSVExwsT","outputId":"e86e8d13-6b4a-4746-cc07-07df53473361"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["By pressing any key to continue you confirm that you have agreed to the FaceForensics terms of use as described at:\n","http://kaldir.vc.in.tum.de/faceforensics/webpage/FaceForensics_TOS.pdf\n","***\n","Press any key to continue, or CTRL-C to exit.\n","\n","Downloading models of dataset \"manipulated_sequences/Deepfakes\"\n","Downloading the first 200 videos\n","Output path: /content/drive/MyDrive/fake_generated_data/face_forensics/manipulated_sequences/Deepfakes/models\n"]},{"output_type":"stream","name":"stderr","text":[" 36%|███▌      | 71/200 [1:31:03<3:04:58, 86.03s/it]"]}]},{"cell_type":"code","source":["!unzip -q /content/deepfake-detection-challenge.zip"],"metadata":{"id":"xxV2i_CmOiPv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!cp -r /content/sample_submission.csv /content/drive/MyDrive/fake_videos/dataset"],"metadata":{"id":"KjWNoVXHO0V3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# code"],"metadata":{"id":"b-D58Nq-PHgn"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"ZbJUJD1JPJgO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import cv2\n","import numpy as np\n","from tqdm import tqdm\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","# Dataset paths\n","REAL_PATH = \"/content/drive/MyDrive/fake_generated_data/face_forensics/original/original_sequences/youtube/c23/videos\"\n","FAKE_PATH = \"/content/drive/MyDrive/fake_generated_data/face_forensics/manipulated_sequences/FaceShifter/c23/videos\"\n","OUTPUT_FRAME_SIZE = (128, 128)  # Frame dimensions\n","FRAME_COUNT = 10  # Number of frames to extract per video\n","MAX_VIDEOS = 700  # Number of videos to process from each category\n","\n","# Function to extract frames from a video\n","def extract_frames(video_path, output_size=(128, 128), frame_count=10):\n","    cap = cv2.VideoCapture(video_path)\n","    frames = []\n","    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","    step = max(total_frames // frame_count, 1)  # Uniform sampling\n","\n","    for i in range(frame_count):\n","        cap.set(cv2.CAP_PROP_POS_FRAMES, i * step)\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        frame = cv2.resize(frame, output_size)\n","        frames.append(frame)\n","    cap.release()\n","    return np.array(frames)\n","\n","# Prepare data and labels\n","data = []\n","labels = []\n","\n","# Process real videos\n","print(\"Processing real videos...\")\n","real_videos = os.listdir(REAL_PATH)[:MAX_VIDEOS]   # Limit to 300 videos\n","for video_file in tqdm(real_videos):\n","    video_path = os.path.join(REAL_PATH, video_file)\n","    frames = extract_frames(video_path, output_size=OUTPUT_FRAME_SIZE, frame_count=FRAME_COUNT)\n","    if len(frames) == FRAME_COUNT:  # Ensure correct frame count\n","        data.append(frames)\n","        labels.append(0)  # Label 0 for real\n","\n","# Process fake videos\n","print(\"Processing fake videos...\")\n","fake_videos = os.listdir(FAKE_PATH)[:MAX_VIDEOS]  # Limit to 300 videos\n","for video_file in tqdm(fake_videos):\n","    video_path = os.path.join(FAKE_PATH, video_file)\n","    frames = extract_frames(video_path, output_size=OUTPUT_FRAME_SIZE, frame_count=FRAME_COUNT)\n","    if len(frames) == FRAME_COUNT:\n","        data.append(frames)\n","        labels.append(1)  # Label 1 for fake\n","\n","# Convert to numpy arrays\n","data = np.array(data)  # Shape: (num_videos, num_frames, 128, 128, 3)\n","labels = np.array(labels)\n","\n","# Split into train, validation, and test sets\n","X_train, X_temp, y_train, y_temp = train_test_split(data, labels, test_size=0.3, random_state=42)\n","X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n","\n","# Normalize data\n","X_train = X_train / 255.0\n","X_val = X_val / 255.0\n","X_test = X_test / 255.0\n","\n","# Convert labels to categorical one hot encoding (real and fake class)\n","y_train = to_categorical(y_train, num_classes=2)\n","y_val = to_categorical(y_val, num_classes=2)\n","y_test = to_categorical(y_test, num_classes=2)\n","\n","print(f\"Data shapes: Train - {X_train.shape}, Validation - {X_val.shape}, Test - {X_test.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HZ3gBKJfPOHn","executionInfo":{"status":"ok","timestamp":1752213892604,"user_tz":-360,"elapsed":1296305,"user":{"displayName":"K M Asifur Rahman","userId":"04755396723086649626"}},"outputId":"66e7945b-dfa5-465f-c0d2-3a4d7c367981"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing real videos...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 200/200 [10:47<00:00,  3.24s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Processing fake videos...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 200/200 [10:44<00:00,  3.22s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Data shapes: Train - (280, 10, 128, 128, 3), Validation - (60, 10, 128, 128, 3), Test - (60, 10, 128, 128, 3)\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","# Augment frames\n","datagen = ImageDataGenerator(\n","    horizontal_flip=True,\n","    rotation_range=10,\n","    zoom_range=0.1,\n","    brightness_range=[0.8, 1.2]\n",")\n","\n","# Function to augment extracted frames\n","def augment_frames(frames):\n","    augmented_frames = []\n","    for frame in frames:\n","        frame = datagen.random_transform(frame)\n","        augmented_frames.append(frame)\n","    return np.array(augmented_frames)\n","\n","# Augment training data\n","augmented_data = []\n","augmented_labels = []\n","\n","for i in range(len(X_train)):\n","    augmented_frames = augment_frames(X_train[i])\n","    augmented_data.append(augmented_frames)\n","    augmented_labels.append(y_train[i])\n","\n","# Combine original and augmented data\n","X_train_augmented = np.concatenate((X_train, np.array(augmented_data)))\n","y_train_augmented = np.concatenate((y_train, np.array(augmented_labels)))\n","\n","print(f\"Augmented Train Data: {X_train_augmented.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"690IQJfdbiGx","executionInfo":{"status":"ok","timestamp":1752213923124,"user_tz":-360,"elapsed":13227,"user":{"displayName":"K M Asifur Rahman","userId":"04755396723086649626"}},"outputId":"1b22596b-c765-45de-efb3-5bf77087a31a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Augmented Train Data: (560, 10, 128, 128, 3)\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.applications import Xception\n","from tensorflow.keras.layers import Dense, Flatten, TimeDistributed, LSTM\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dropout\n","\n","# Define model\n","def build_improved_model(input_shape=(FRAME_COUNT, 128, 128, 3)):\n","    model = Sequential([\n","        TimeDistributed(Xception(weights='imagenet', include_top=False, input_shape=(128, 128, 3))),\n","        TimeDistributed(Flatten()),\n","        Dropout(0.5),  # Add dropout for regularization\n","        LSTM(128, return_sequences=False),\n","        Dropout(0.5),  # Add dropout\n","        Dense(64, activation='relu'),\n","        Dense(2, activation='softmax')\n","    ])\n","    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n","                  loss='categorical_crossentropy',\n","                  metrics=['accuracy'])\n","    return model\n","\n","model = build_improved_model()\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":456},"id":"G_l-eWgobkso","executionInfo":{"status":"ok","timestamp":1752213935885,"user_tz":-360,"elapsed":4262,"user":{"displayName":"K M Asifur Rahman","userId":"04755396723086649626"}},"outputId":"752fdae5-c70e-4483-d502-688f8804f48b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n","\u001b[1m83683744/83683744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ time_distributed                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ time_distributed_1              │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ time_distributed                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ time_distributed_1              │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m20,861,480\u001b[0m (79.58 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,861,480</span> (79.58 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m20,806,952\u001b[0m (79.37 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,806,952</span> (79.37 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m54,528\u001b[0m (213.00 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">54,528</span> (213.00 KB)\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"code","source":["from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n","\n","# Model checkpoint to save the best model in .keras format\n","checkpoint = ModelCheckpoint(\n","    \"deepfake_detection_model.keras\",  # Change to .keras\n","    monitor=\"val_accuracy\",\n","    save_best_only=True,\n","    verbose=1\n",")\n","\n","# Reduce learning rate on plateau\n","lr_scheduler = ReduceLROnPlateau(\n","    monitor=\"val_loss\",\n","    factor=0.5,\n","    patience=3,\n","    verbose=1\n",")\n","\n","# Train the model\n","history = model.fit(\n","    X_train_augmented, y_train_augmented,\n","    validation_data=(X_val, y_val),\n","    epochs=40,\n","    batch_size=10,\n","    callbacks=[checkpoint, lr_scheduler]\n",")\n","model.save(\"/content/drive/MyDrive/fake_generated_data/deepfake_detection_model_lstm.keras\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1-xtXb7ebrE1","executionInfo":{"status":"ok","timestamp":1752216952733,"user_tz":-360,"elapsed":2961908,"user":{"displayName":"K M Asifur Rahman","userId":"04755396723086649626"}},"outputId":"366d358f-ae0d-443b-ed06-57c4e8077eb6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/40\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 782ms/step - accuracy: 0.5141 - loss: 0.7538\n","Epoch 1: val_accuracy improved from -inf to 0.48333, saving model to deepfake_detection_model.keras\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 1s/step - accuracy: 0.5139 - loss: 0.7536 - val_accuracy: 0.4833 - val_loss: 0.6957 - learning_rate: 1.0000e-04\n","Epoch 2/40\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 812ms/step - accuracy: 0.5159 - loss: 0.7121\n","Epoch 2: val_accuracy did not improve from 0.48333\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 836ms/step - accuracy: 0.5159 - loss: 0.7119 - val_accuracy: 0.3167 - val_loss: 0.6997 - learning_rate: 1.0000e-04\n","Epoch 3/40\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 805ms/step - accuracy: 0.5262 - loss: 0.6919\n","Epoch 3: val_accuracy did not improve from 0.48333\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 829ms/step - accuracy: 0.5256 - loss: 0.6921 - val_accuracy: 0.3667 - val_loss: 0.7015 - learning_rate: 1.0000e-04\n","Epoch 4/40\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 799ms/step - accuracy: 0.5026 - loss: 0.7089\n","Epoch 4: val_accuracy did not improve from 0.48333\n","\n","Epoch 4: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 823ms/step - accuracy: 0.5028 - loss: 0.7088 - val_accuracy: 0.3833 - val_loss: 0.7016 - learning_rate: 1.0000e-04\n","Epoch 5/40\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 809ms/step - accuracy: 0.5244 - loss: 0.7103\n","Epoch 5: val_accuracy did not improve from 0.48333\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 833ms/step - accuracy: 0.5243 - loss: 0.7103 - val_accuracy: 0.4500 - val_loss: 0.6986 - learning_rate: 5.0000e-05\n","Epoch 6/40\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 797ms/step - accuracy: 0.4734 - loss: 0.7085\n","Epoch 6: val_accuracy did not improve from 0.48333\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 818ms/step - accuracy: 0.4735 - loss: 0.7085 - val_accuracy: 0.4000 - val_loss: 0.7040 - learning_rate: 5.0000e-05\n","Epoch 7/40\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 807ms/step - accuracy: 0.5231 - loss: 0.6881\n","Epoch 7: val_accuracy did not improve from 0.48333\n","\n","Epoch 7: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 831ms/step - accuracy: 0.5231 - loss: 0.6882 - val_accuracy: 0.3667 - val_loss: 0.7073 - learning_rate: 5.0000e-05\n","Epoch 8/40\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 806ms/step - accuracy: 0.6022 - loss: 0.6779\n","Epoch 8: val_accuracy did not improve from 0.48333\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 830ms/step - accuracy: 0.6014 - loss: 0.6780 - val_accuracy: 0.3333 - val_loss: 0.7135 - learning_rate: 2.5000e-05\n","Epoch 9/40\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 808ms/step - accuracy: 0.5304 - loss: 0.6878\n","Epoch 9: val_accuracy did not improve from 0.48333\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 829ms/step - accuracy: 0.5308 - loss: 0.6878 - val_accuracy: 0.3667 - val_loss: 0.7098 - learning_rate: 2.5000e-05\n","Epoch 10/40\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 808ms/step - accuracy: 0.5535 - loss: 0.6956\n","Epoch 10: val_accuracy did not improve from 0.48333\n","\n","Epoch 10: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 832ms/step - accuracy: 0.5528 - loss: 0.6956 - val_accuracy: 0.2833 - val_loss: 0.7326 - learning_rate: 2.5000e-05\n","Epoch 11/40\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 807ms/step - accuracy: 0.5370 - loss: 0.6869\n","Epoch 11: val_accuracy did not improve from 0.48333\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 831ms/step - accuracy: 0.5366 - loss: 0.6869 - val_accuracy: 0.3333 - val_loss: 0.7287 - learning_rate: 1.2500e-05\n","Epoch 12/40\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 799ms/step - accuracy: 0.5015 - loss: 0.6937\n","Epoch 12: val_accuracy did not improve from 0.48333\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 822ms/step - accuracy: 0.5018 - loss: 0.6936 - val_accuracy: 0.4167 - val_loss: 0.7084 - learning_rate: 1.2500e-05\n","Epoch 13/40\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 808ms/step - accuracy: 0.4762 - loss: 0.6963\n","Epoch 13: val_accuracy did not improve from 0.48333\n","\n","Epoch 13: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 829ms/step - accuracy: 0.4768 - loss: 0.6962 - val_accuracy: 0.3333 - val_loss: 0.7325 - learning_rate: 1.2500e-05\n","Epoch 14/40\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 807ms/step - accuracy: 0.5356 - loss: 0.6907\n","Epoch 14: val_accuracy did not improve from 0.48333\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 830ms/step - accuracy: 0.5361 - loss: 0.6906 - val_accuracy: 0.3167 - val_loss: 0.7544 - learning_rate: 6.2500e-06\n","Epoch 15/40\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 798ms/step - accuracy: 0.5340 - loss: 0.6873\n","Epoch 15: val_accuracy did not improve from 0.48333\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 820ms/step - accuracy: 0.5338 - loss: 0.6874 - val_accuracy: 0.3333 - val_loss: 0.7630 - learning_rate: 6.2500e-06\n","Epoch 16/40\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 810ms/step - accuracy: 0.5229 - loss: 0.6956\n","Epoch 16: val_accuracy did not improve from 0.48333\n","\n","Epoch 16: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 833ms/step - accuracy: 0.5232 - loss: 0.6955 - val_accuracy: 0.3167 - val_loss: 0.7608 - learning_rate: 6.2500e-06\n","Epoch 17/40\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 809ms/step - accuracy: 0.5366 - loss: 0.6873\n","Epoch 17: val_accuracy did not improve from 0.48333\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 831ms/step - accuracy: 0.5364 - loss: 0.6874 - val_accuracy: 0.3000 - val_loss: 0.7844 - learning_rate: 3.1250e-06\n","Epoch 18/40\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 810ms/step - accuracy: 0.4929 - loss: 0.6913\n","Epoch 18: val_accuracy did not improve from 0.48333\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 832ms/step - accuracy: 0.4932 - loss: 0.6913 - val_accuracy: 0.3667 - val_loss: 0.7495 - learning_rate: 3.1250e-06\n","Epoch 19/40\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 799ms/step - accuracy: 0.5792 - loss: 0.6791\n","Epoch 19: val_accuracy did not improve from 0.48333\n","\n","Epoch 19: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 821ms/step - accuracy: 0.5791 - loss: 0.6792 - val_accuracy: 0.2667 - val_loss: 0.7945 - learning_rate: 3.1250e-06\n","Epoch 20/40\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 810ms/step - accuracy: 0.5164 - loss: 0.6886\n","Epoch 20: val_accuracy did not improve from 0.48333\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 834ms/step - accuracy: 0.5164 - loss: 0.6886 - val_accuracy: 0.3333 - val_loss: 0.7456 - learning_rate: 1.5625e-06\n","Epoch 21/40\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 808ms/step - accuracy: 0.5166 - loss: 0.6821\n","Epoch 21: val_accuracy did not improve from 0.48333\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 832ms/step - accuracy: 0.5170 - loss: 0.6822 - val_accuracy: 0.2833 - val_loss: 0.8011 - learning_rate: 1.5625e-06\n","Epoch 22/40\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 810ms/step - accuracy: 0.5448 - loss: 0.6885\n","Epoch 22: val_accuracy did not improve from 0.48333\n","\n","Epoch 22: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 834ms/step - accuracy: 0.5449 - loss: 0.6883 - val_accuracy: 0.3333 - val_loss: 0.7663 - learning_rate: 1.5625e-06\n","Epoch 23/40\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 800ms/step - accuracy: 0.5218 - loss: 0.6804\n","Epoch 23: val_accuracy did not improve from 0.48333\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 821ms/step - accuracy: 0.5220 - loss: 0.6805 - val_accuracy: 0.2667 - val_loss: 0.7961 - learning_rate: 7.8125e-07\n","Epoch 24/40\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 803ms/step - accuracy: 0.5225 - loss: 0.6796\n","Epoch 24: val_accuracy did not improve from 0.48333\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 826ms/step - accuracy: 0.5229 - loss: 0.6795 - val_accuracy: 0.2833 - val_loss: 0.7929 - learning_rate: 7.8125e-07\n","Epoch 25/40\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 810ms/step - accuracy: 0.5785 - loss: 0.6698\n","Epoch 25: val_accuracy did not improve from 0.48333\n","\n","Epoch 25: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 832ms/step - accuracy: 0.5781 - loss: 0.6699 - val_accuracy: 0.3333 - val_loss: 0.7748 - learning_rate: 7.8125e-07\n","Epoch 26/40\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 810ms/step - accuracy: 0.5516 - loss: 0.6806\n","Epoch 26: val_accuracy did not improve from 0.48333\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 834ms/step - accuracy: 0.5517 - loss: 0.6806 - val_accuracy: 0.2667 - val_loss: 0.8115 - learning_rate: 3.9062e-07\n","Epoch 27/40\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 800ms/step - accuracy: 0.5812 - loss: 0.6749\n","Epoch 27: val_accuracy did not improve from 0.48333\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 824ms/step - accuracy: 0.5808 - loss: 0.6750 - val_accuracy: 0.3000 - val_loss: 0.7757 - learning_rate: 3.9062e-07\n","Epoch 28/40\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 805ms/step - accuracy: 0.5434 - loss: 0.6861\n","Epoch 28: val_accuracy did not improve from 0.48333\n","\n","Epoch 28: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-07.\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 827ms/step - accuracy: 0.5436 - loss: 0.6860 - val_accuracy: 0.3000 - val_loss: 0.7823 - learning_rate: 3.9062e-07\n","Epoch 29/40\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 811ms/step - accuracy: 0.5236 - loss: 0.6789\n","Epoch 29: val_accuracy did not improve from 0.48333\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 835ms/step - accuracy: 0.5235 - loss: 0.6791 - val_accuracy: 0.2667 - val_loss: 0.8092 - learning_rate: 1.9531e-07\n","Epoch 30/40\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 811ms/step - accuracy: 0.5268 - loss: 0.6896\n","Epoch 30: val_accuracy did not improve from 0.48333\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 833ms/step - accuracy: 0.5270 - loss: 0.6896 - val_accuracy: 0.2667 - val_loss: 0.8014 - learning_rate: 1.9531e-07\n","Epoch 31/40\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 811ms/step - accuracy: 0.5944 - loss: 0.6762\n","Epoch 31: val_accuracy did not improve from 0.48333\n","\n","Epoch 31: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-08.\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 833ms/step - accuracy: 0.5943 - loss: 0.6762 - val_accuracy: 0.2833 - val_loss: 0.7864 - learning_rate: 1.9531e-07\n","Epoch 32/40\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 801ms/step - accuracy: 0.5125 - loss: 0.6896\n","Epoch 32: val_accuracy did not improve from 0.48333\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 825ms/step - accuracy: 0.5128 - loss: 0.6895 - val_accuracy: 0.2667 - val_loss: 0.8254 - learning_rate: 9.7656e-08\n","Epoch 33/40\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 813ms/step - accuracy: 0.5524 - loss: 0.6846\n","Epoch 33: val_accuracy did not improve from 0.48333\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 835ms/step - accuracy: 0.5523 - loss: 0.6846 - val_accuracy: 0.2833 - val_loss: 0.8423 - learning_rate: 9.7656e-08\n","Epoch 34/40\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 809ms/step - accuracy: 0.5546 - loss: 0.6818\n","Epoch 34: val_accuracy did not improve from 0.48333\n","\n","Epoch 34: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-08.\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 833ms/step - accuracy: 0.5544 - loss: 0.6818 - val_accuracy: 0.2667 - val_loss: 0.8177 - learning_rate: 9.7656e-08\n","Epoch 35/40\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 814ms/step - accuracy: 0.5141 - loss: 0.6826\n","Epoch 35: val_accuracy did not improve from 0.48333\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 837ms/step - accuracy: 0.5146 - loss: 0.6826 - val_accuracy: 0.2833 - val_loss: 0.8424 - learning_rate: 4.8828e-08\n","Epoch 36/40\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 811ms/step - accuracy: 0.5646 - loss: 0.6722\n","Epoch 36: val_accuracy did not improve from 0.48333\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 836ms/step - accuracy: 0.5644 - loss: 0.6723 - val_accuracy: 0.3000 - val_loss: 0.7720 - learning_rate: 4.8828e-08\n","Epoch 37/40\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 813ms/step - accuracy: 0.5583 - loss: 0.6831\n","Epoch 37: val_accuracy did not improve from 0.48333\n","\n","Epoch 37: ReduceLROnPlateau reducing learning rate to 2.4414061883248905e-08.\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 837ms/step - accuracy: 0.5579 - loss: 0.6831 - val_accuracy: 0.2667 - val_loss: 0.7997 - learning_rate: 4.8828e-08\n","Epoch 38/40\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 813ms/step - accuracy: 0.4773 - loss: 0.6912\n","Epoch 38: val_accuracy did not improve from 0.48333\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 835ms/step - accuracy: 0.4781 - loss: 0.6910 - val_accuracy: 0.3000 - val_loss: 0.8028 - learning_rate: 2.4414e-08\n","Epoch 39/40\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 812ms/step - accuracy: 0.5307 - loss: 0.6832\n","Epoch 39: val_accuracy did not improve from 0.48333\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 834ms/step - accuracy: 0.5309 - loss: 0.6832 - val_accuracy: 0.2667 - val_loss: 0.8322 - learning_rate: 2.4414e-08\n","Epoch 40/40\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 801ms/step - accuracy: 0.5351 - loss: 0.6881\n","Epoch 40: val_accuracy did not improve from 0.48333\n","\n","Epoch 40: ReduceLROnPlateau reducing learning rate to 1.2207030941624453e-08.\n","\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 823ms/step - accuracy: 0.5351 - loss: 0.6881 - val_accuracy: 0.2500 - val_loss: 0.8166 - learning_rate: 2.4414e-08\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import classification_report, accuracy_score\n","\n","# Load the best saved model\n","from tensorflow.keras.models import load_model\n","model = load_model('deepfake_detection_model.keras')\n","\n","# Evaluate on test set\n","y_pred = model.predict(X_test)\n","y_pred_classes = np.argmax(y_pred, axis=1)\n","y_true = np.argmax(y_test, axis=1)\n","\n","# Metrics\n","accuracy = accuracy_score(y_true, y_pred_classes)\n","print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n","\n","# Precision, Recall, F1-Score\n","print(\"Classification Report:\")\n","print(classification_report(y_true, y_pred_classes, target_names=['REAL', 'FAKE']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1kaoTi9jbwEe","executionInfo":{"status":"ok","timestamp":1752217053939,"user_tz":-360,"elapsed":30166,"user":{"displayName":"K M Asifur Rahman","userId":"04755396723086649626"}},"outputId":"dbdfebd2-100a-4d36-b54d-645c0ae0ec1d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 12s/step\n","Test Accuracy: 46.67%\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","        REAL       0.51      0.67      0.58        33\n","        FAKE       0.35      0.22      0.27        27\n","\n","    accuracy                           0.47        60\n","   macro avg       0.43      0.44      0.43        60\n","weighted avg       0.44      0.47      0.44        60\n","\n"]}]},{"cell_type":"markdown","source":["face/f++ dataset\n","Test Accuracy: 46.67%\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","        REAL       0.52      0.33      0.41        33\n","        FAKE       0.44      0.63      0.52        27\n","\n","    accuracy                           0.47        60\n","   macro avg       0.48      0.48      0.46        60\n","weighted avg       0.48      0.47      0.46        60\n","\n","\n","face forensics:\n","\n","Test Accuracy: 46.67%\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","        REAL       0.51      0.67      0.58        33\n","        FAKE       0.35      0.22      0.27        27\n","\n","    accuracy                           0.47        60\n","   macro avg       0.43      0.44      0.43        60\n","weighted avg       0.44      0.47      0.44        60\n","\n","\n","siglip:\n","\n","Test Accuracy: 45.00%\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","        REAL       0.00      0.00      0.00        33\n","        FAKE       0.45      1.00      0.62        27\n","\n","    accuracy                           0.45        60\n","   macro avg       0.23      0.50      0.31        60\n","weighted avg       0.20      0.45      0.28        60\n","\n","siglip-fine tune acc = 0.60 on batch 32 epoch 6"],"metadata":{"id":"hm4R_ejcRZwb"}},{"cell_type":"code","source":["loaded_model = load_model('deepfake_detection_model.keras')"],"metadata":{"id":"G3I4OaBnrAhA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.models import load_model\n","\n","# Load the model for real-time detection\n","\n","\n","def predict_video(video_path, model, output_size=(128, 128), frame_count=10):\n","    frames = extract_frames(video_path, output_size, frame_count)\n","    frames = frames / 255.0  # Normalize\n","    frames = np.expand_dims(frames, axis=0)  # Add batch dimension\n","    prediction = model.predict(frames)\n","    label = \"FAKE\" if np.argmax(prediction) == 1 else \"REAL\"\n","    confidence = prediction[0][np.argmax(prediction)]\n","    print(f\"Prediction: {label} (Confidence: {confidence:.2f})\")\n","\n","REAL_PATH = \"/content/drive/MyDrive/fake_videos/out1.mp4\"\n","FAKE_PATH = \"/kaggle/input/deep-fake-detection-dfd-entire-original-dataset/DFD_manipulated_sequences/DFD_manipulated_sequences\"\n","# Test prediction on a video\n","# real_sample_path = os.path.join(REAL_PATH, \"/kaggle/input/deepfake-testing-videos/model1.mp4\")  # Replace with real video path\n","# fake_sample_path = os.path.join(FAKE_PATH, \"/kaggle/input/deepfake-testing-videos/modeloutput1.mp4\")  # Replace with fake video path\n","\n","real_sample_path = \"/content/drive/MyDrive/fake_videos/smile_blink.mp4\"\n","fake_sample_path = \"/content/drive/MyDrive/fake_videos/spoofed_samiul.mp4\"\n","print(\"Fake Video Prediction:\")\n","predict_video(fake_sample_path,loaded_model)\n","\n","print(\"Real Video Prediction:\")\n","predict_video(real_sample_path,loaded_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GWnlrS9rb1DW","executionInfo":{"status":"ok","timestamp":1751821129058,"user_tz":-360,"elapsed":3858,"user":{"displayName":"K M Asifur Rahman","userId":"04755396723086649626"}},"outputId":"02078547-df6a-4222-bc76-8a234795b1bb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Fake Video Prediction:\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n","Prediction: FAKE (Confidence: 0.67)\n","Real Video Prediction:\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\n","Prediction: FAKE (Confidence: 0.74)\n"]}]}]}